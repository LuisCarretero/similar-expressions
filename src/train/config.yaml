model:
  encoder:
    size_hidden: 256
    architecture: residual-parameterized
    depth: 4
    width: 1024
  z_size: 48
  decoder:
    z_slice: [0, -1]
    size_hidden: 128
    architecture: residual-parameterized
    depth: 4
    width: 1024
  value_decoder:
    z_slice: [0, -1]
    architecture: residual-parameterized
    depth: 0
    width: 0
  io_format:
    seq_len: 15
    token_cnt: 12  # nbin+nuna+4(CON, x1, END; const)
    val_points: 100

training:
  batch_size: 512
  epochs: 100
  valid_split: 0.1
  dataset_name: dataset_250110_2
  dataset_len_limit: null
  wandb_settings:
    project: simexp-03 
  mode: autoencoding  # value_prediction, autoencoding, mixed, encoding
  loss:
    ae_weight: 1
    kl_weight: 0  # beta in beta-VAE
    syntax_weight: 0.5  # Relative weighting between syntax and constants
    contrastive:
      mode: piecewise  # null, total, piecewise
      dimensions: [0, 25]  # 25 intervals
      weight: 1
      scale: 1
      similarity_threshold: 0.2
  value_transform:
    mapping: null  # [null, arcsinh]
    bias: null  # [null, dataset, sample]
    scale: null  # [null, dataset-std, sample-std, dataset-range, sample-range]
  sampling:
    prior_std: 1
    eps: 1e-2
  optimizer:
    lr: 3e-4
    clip: 5
    scheduler_factor: 0.1
    scheduler_patience: 10
    scheduler_threshold: 2e-4
    scheduler_min_lr: 0
  early_stopping:
    enabled: true
    min_delta: 0
    patience: 15
  performance_metric: valid/loss
  kl_anneal:
    schedule: sigmoid
    midpoint: 0.4
    steepness: 15
  values_init_bias: false
  use_grammar_mask: false

