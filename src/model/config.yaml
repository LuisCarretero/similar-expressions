model:
  encoder:
    size_hidden: 256
    architecture: residual-parameterized
    depth: 4
    width: 1024
  z_size: 32
  decoder:
    z_slice: [0, -1]
    size_hidden: 128
    architecture: residual-parameterized
    depth: 3
    width: 1024
  value_decoder:
    z_slice: [0, -1]
    architecture: residual-parameterized
    depth: 3
    width: 1024
  io_format:
    seq_len: 15
    token_cnt: 14
    val_points: 100

training:
  batch_size: 512
  epochs: 10
  valid_split: 0.1
  dataset_len_limit: 569
  mode: encoding  # value_prediction, autoencoding, mixed, encoding
  criterion:
    ae_weight: 0
    kl_weight: 0
    syntax_weight: 0.5
    contrastive_weight: 1
  value_transform:
    mapping: null  # [null, arcsinh]
    bias: null  # [null, dataset, sample]
    scale: null  # [null, dataset-std, sample-std, dataset-range, sample-range]
  sampling:
    prior_std: 1
    eps: 0
  optimizer:
    lr: 3e-4
    clip: 5
    scheduler_factor: 0.1
    scheduler_patience: 10
    scheduler_threshold: 2e-4
    scheduler_min_lr: 0
  performance_metric: train/loss
  kl_anneal:
    schedule: sigmoid
    midpoint: 0.4
    steepness: 15
  values_init_bias: false
  use_grammar_mask: false
